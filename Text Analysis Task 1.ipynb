{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "## HAN YAOWEI (6930-31-2659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prepare 10 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = [''] * 10\n",
    "doc[0] = 'How does the Surface Pro himself 4 compare with iPad Pro?'\n",
    "doc[1] = 'Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?'\n",
    "doc[2] = 'Should I have a hair transplant at age 24? How much would it cost?'\n",
    "doc[3] = 'How much cost does hair transplant require?'\n",
    "doc[4] = 'What but is the best way to send money from China to the US?'\n",
    "doc[5] = 'What you send money to China?'\n",
    "doc[6] = 'Which food not emulsifiers?'\n",
    "doc[7] = 'What foods fibre?'\n",
    "doc[8] = 'How \"aberystwyth\" start reading?'\n",
    "doc[9] = 'How their can I start reading?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set based similarity (Only report the results which not equal 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (0) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def cleaning_text(text):\n",
    "    remove = str.maketrans('','',string.punctuation) \n",
    "    text = text.translate(remove)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn # import lemmatize\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    word=word.lower()\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "en_stop = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(word, stopwordset):\n",
    "    if word in stopwordset:\n",
    "        return None\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'doe', 'surface', 'ipad', 'compare', '4', 'pro'}, 1: {'m3', 'surface', 'microsoft', 'home', 'core', '4', 'i3', 'pro', 'choose'}, 2: {'cost', 'would', 'age', 'transplant', '24', 'much', 'hair'}, 3: {'doe', 'cost', 'require', 'transplant', 'much', 'hair'}, 4: {'china', 'us', 'way', 'money', 'best', 'send'}, 5: {'money', 'china', 'send'}, 6: {'food', 'emulsifier'}, 7: {'food', 'fibre'}, 8: {'aberystwyth', 'reading', 'start'}, 9: {'reading', 'start'}}\n"
     ]
    }
   ],
   "source": [
    "set_doc = dict()\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    text = cleaning_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = [lemmatize_word(word) for word in tokens]\n",
    "    tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
    "    tokens = [word for word in tokens if word is not None]\n",
    "    return tokens\n",
    "\n",
    "preprocessed_docs = [preprocessing_text(text) for text in doc]\n",
    "for i in range(10):\n",
    "    set_doc[i] = set(preprocessed_docs[i])\n",
    "print(set_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Jaccard index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jaccard_similarity between doc0 and doc1 is 0.25\n",
      "The jaccard_similarity between doc0 and doc3 is 0.09090909090909094\n",
      "The jaccard_similarity between doc2 and doc3 is 0.4444444444444444\n",
      "The jaccard_similarity between doc4 and doc5 is 0.5\n",
      "The jaccard_similarity between doc6 and doc7 is 0.33333333333333337\n",
      "The jaccard_similarity between doc8 and doc9 is 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(i + 1, 10):\n",
    "        if 1 - jaccard_distance(set_doc[i], set_doc[j]) != 0:\n",
    "            print('The jaccard_similarity between doc{0} and doc{1} is {2}'.format(i, j, 1 - jaccard_distance(set_doc[i], set_doc[j])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that since jaccard_similarity defines the similarity and diversity of two sets, the higher the proportion of intersections, the more similar the two documents.\n",
    "\n",
    "However, if one set's size is too large, the value is small, no matter how large the intersection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Sørensen–Dice coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_similarity(set_a, set_b):\n",
    "    num_intersection =  len(set.intersection(set_a, set_b))\n",
    "    sum_nums = len(set_a) + len(set_b)\n",
    "    try:\n",
    "        return 2 * num_intersection / sum_nums\n",
    "    except ZeroDivisionError:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dice_similarity between doc0 and doc1 is 0.4\n",
      "The dice_similarity between doc0 and doc3 is 0.16666666666666666\n",
      "The dice_similarity between doc2 and doc3 is 0.6153846153846154\n",
      "The dice_similarity between doc4 and doc5 is 0.6666666666666666\n",
      "The dice_similarity between doc6 and doc7 is 0.5\n",
      "The dice_similarity between doc8 and doc9 is 0.8\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(i + 1, 10):\n",
    "        if dice_similarity(set_doc[i], set_doc[j]) != 0:\n",
    "            print('The dice_similarity between doc{0} and doc{1} is {2}'.format(i, j, dice_similarity(set_doc[i], set_doc[j])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the Jaccard Index's above problem, Sørensen–Dice coefficient sets the denominator as the average value of the two sets.\n",
    "Thus, we can see that dice_similarity seems more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Overlap coefficient (Szymkiewicz–Simpson coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpson_similarity(list_a, list_b):\n",
    "    num_intersection = len(set.intersection(set(list_a), set(list_b)))\n",
    "    min_num = min(len(set(list_a)), len(set(list_b)))\n",
    "    try:\n",
    "        return num_intersection / min_num\n",
    "    except ZeroDivisionError:\n",
    "        if num_intersection == 0:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The simpson_similarity between doc0 and doc1 is 0.5\n",
      "The simpson_similarity between doc0 and doc3 is 0.16666666666666666\n",
      "The simpson_similarity between doc2 and doc3 is 0.6666666666666666\n",
      "The simpson_similarity between doc4 and doc5 is 1.0\n",
      "The simpson_similarity between doc6 and doc7 is 0.5\n",
      "The simpson_similarity between doc8 and doc9 is 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(i + 1, 10):\n",
    "        if simpson_similarity(set_doc[i], set_doc[j]) != 0:\n",
    "            print('The simpson_similarity between doc{0} and doc{1} is {2}'.format(i, j, simpson_similarity(set_doc[i], set_doc[j])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overlap coefficient is defined as the size of the intersection divided by the smaller of the size of the two sets.\n",
    "Thus, we can see that if words in one document totally appear in another document, the overlap coefficient is 1.\n",
    "It looks like it cannot reflect the similarity very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vector based similarity (Only report the results which not equal 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (0) Transfer each document to vectorized representation using BoW. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 2 0\n",
      "  0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 0 2 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
      "  0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      "  1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      "  0 1 0 0 0 2 0 2 0 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(doc)\n",
    "vector = vectorizer.transform(doc)\n",
    "BoW = vector.toarray()\n",
    "print(BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Minkowski distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1).a Manhattan distance when p = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The manhattan_distance between doc0 and doc1 is 19.0\n",
      "The manhattan_distance between doc0 and doc2 is 20.0\n",
      "The manhattan_distance between doc0 and doc3 is 13.0\n",
      "The manhattan_distance between doc0 and doc4 is 22.0\n",
      "The manhattan_distance between doc0 and doc5 is 16.0\n",
      "The manhattan_distance between doc0 and doc6 is 14.0\n",
      "The manhattan_distance between doc0 and doc7 is 13.0\n",
      "The manhattan_distance between doc0 and doc8 is 12.0\n",
      "The manhattan_distance between doc0 and doc9 is 13.0\n",
      "The manhattan_distance between doc1 and doc2 is 25.0\n",
      "The manhattan_distance between doc1 and doc3 is 20.0\n",
      "The manhattan_distance between doc1 and doc4 is 27.0\n",
      "The manhattan_distance between doc1 and doc5 is 19.0\n",
      "The manhattan_distance between doc1 and doc6 is 15.0\n",
      "The manhattan_distance between doc1 and doc7 is 16.0\n",
      "The manhattan_distance between doc1 and doc8 is 17.0\n",
      "The manhattan_distance between doc1 and doc9 is 18.0\n",
      "The manhattan_distance between doc2 and doc3 is 9.0\n",
      "The manhattan_distance between doc2 and doc4 is 26.0\n",
      "The manhattan_distance between doc2 and doc5 is 18.0\n",
      "The manhattan_distance between doc2 and doc6 is 16.0\n",
      "The manhattan_distance between doc2 and doc7 is 15.0\n",
      "The manhattan_distance between doc2 and doc8 is 14.0\n",
      "The manhattan_distance between doc2 and doc9 is 15.0\n",
      "The manhattan_distance between doc3 and doc4 is 21.0\n",
      "The manhattan_distance between doc3 and doc5 is 13.0\n",
      "The manhattan_distance between doc3 and doc6 is 11.0\n",
      "The manhattan_distance between doc3 and doc7 is 10.0\n",
      "The manhattan_distance between doc3 and doc8 is 9.0\n",
      "The manhattan_distance between doc3 and doc9 is 10.0\n",
      "The manhattan_distance between doc4 and doc5 is 10.0\n",
      "The manhattan_distance between doc4 and doc6 is 18.0\n",
      "The manhattan_distance between doc4 and doc7 is 15.0\n",
      "The manhattan_distance between doc4 and doc8 is 18.0\n",
      "The manhattan_distance between doc4 and doc9 is 19.0\n",
      "The manhattan_distance between doc5 and doc6 is 10.0\n",
      "The manhattan_distance between doc5 and doc7 is 7.0\n",
      "The manhattan_distance between doc5 and doc8 is 10.0\n",
      "The manhattan_distance between doc5 and doc9 is 11.0\n",
      "The manhattan_distance between doc6 and doc7 is 7.0\n",
      "The manhattan_distance between doc6 and doc8 is 8.0\n",
      "The manhattan_distance between doc6 and doc9 is 9.0\n",
      "The manhattan_distance between doc7 and doc8 is 7.0\n",
      "The manhattan_distance between doc7 and doc9 is 8.0\n",
      "The manhattan_distance between doc8 and doc9 is 3.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(i + 1, 10):\n",
    "        if distance.minkowski(BoW[i], BoW[j], 1) != 0:\n",
    "            print('The manhattan_distance between doc{0} and doc{1} is {2}'.format(i, j, distance.minkowski(BoW[i], BoW[j], 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manhattan distance seems not a good metric since there seems no limit to the scale of distance, and we cannot directly get the result of how these two documents similar or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1).b Euclidean distance when p = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The euclidean_distance between doc0 and doc1 is 4.58257569495584\n",
      "The euclidean_distance between doc0 and doc2 is 4.69041575982343\n",
      "The euclidean_distance between doc0 and doc3 is 3.872983346207417\n",
      "The euclidean_distance between doc0 and doc4 is 5.0990195135927845\n",
      "The euclidean_distance between doc0 and doc5 is 4.242640687119285\n",
      "The euclidean_distance between doc0 and doc6 is 4.0\n",
      "The euclidean_distance between doc0 and doc7 is 3.872983346207417\n",
      "The euclidean_distance between doc0 and doc8 is 3.7416573867739413\n",
      "The euclidean_distance between doc0 and doc9 is 3.872983346207417\n",
      "The euclidean_distance between doc1 and doc2 is 5.196152422706632\n",
      "The euclidean_distance between doc1 and doc3 is 4.69041575982343\n",
      "The euclidean_distance between doc1 and doc4 is 5.744562646538029\n",
      "The euclidean_distance between doc1 and doc5 is 4.58257569495584\n",
      "The euclidean_distance between doc1 and doc6 is 4.123105625617661\n",
      "The euclidean_distance between doc1 and doc7 is 4.242640687119285\n",
      "The euclidean_distance between doc1 and doc8 is 4.358898943540674\n",
      "The euclidean_distance between doc1 and doc9 is 4.47213595499958\n",
      "The euclidean_distance between doc2 and doc3 is 3.0\n",
      "The euclidean_distance between doc2 and doc4 is 5.477225575051661\n",
      "The euclidean_distance between doc2 and doc5 is 4.242640687119285\n",
      "The euclidean_distance between doc2 and doc6 is 4.0\n",
      "The euclidean_distance between doc2 and doc7 is 3.872983346207417\n",
      "The euclidean_distance between doc2 and doc8 is 3.7416573867739413\n",
      "The euclidean_distance between doc2 and doc9 is 3.872983346207417\n",
      "The euclidean_distance between doc3 and doc4 is 5.0\n",
      "The euclidean_distance between doc3 and doc5 is 3.605551275463989\n",
      "The euclidean_distance between doc3 and doc6 is 3.3166247903554\n",
      "The euclidean_distance between doc3 and doc7 is 3.1622776601683795\n",
      "The euclidean_distance between doc3 and doc8 is 3.0\n",
      "The euclidean_distance between doc3 and doc9 is 3.1622776601683795\n",
      "The euclidean_distance between doc4 and doc5 is 3.4641016151377544\n",
      "The euclidean_distance between doc4 and doc6 is 4.69041575982343\n",
      "The euclidean_distance between doc4 and doc7 is 4.358898943540674\n",
      "The euclidean_distance between doc4 and doc8 is 4.69041575982343\n",
      "The euclidean_distance between doc4 and doc9 is 4.795831523312719\n",
      "The euclidean_distance between doc5 and doc6 is 3.1622776601683795\n",
      "The euclidean_distance between doc5 and doc7 is 2.6457513110645907\n",
      "The euclidean_distance between doc5 and doc8 is 3.1622776601683795\n",
      "The euclidean_distance between doc5 and doc9 is 3.3166247903554\n",
      "The euclidean_distance between doc6 and doc7 is 2.6457513110645907\n",
      "The euclidean_distance between doc6 and doc8 is 2.8284271247461903\n",
      "The euclidean_distance between doc6 and doc9 is 3.0\n",
      "The euclidean_distance between doc7 and doc8 is 2.6457513110645907\n",
      "The euclidean_distance between doc7 and doc9 is 2.8284271247461903\n",
      "The euclidean_distance between doc8 and doc9 is 1.7320508075688772\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(i + 1, 10):\n",
    "        if distance.minkowski(BoW[i], BoW[j], 2) != 0:\n",
    "            print('The euclidean_distance between doc{0} and doc{1} is {2}'.format(i, j, distance.minkowski(BoW[i], BoW[j], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to manhattan distance, there seems no limit to the scale of distance, and we cannot directly get the result of how these two documents similar or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(x, y, norm=False):\n",
    "    assert len(x) == len(y), \"len(x) != len(y)\"\n",
    "    res = np.array([[x[i] * y[i], x[i] * x[i], y[i] * y[i]] for i in range(len(x))])\n",
    "    cos = sum(res[:, 0]) / (np.sqrt(sum(res[:, 1])) * np.sqrt(sum(res[:, 2])))\n",
    "\n",
    "    return 0.5 * cos + 0.5 if norm else cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cos_similarity between doc0 and doc1 is 0.223606797749979\n",
      "The cos_similarity between doc0 and doc2 is 0.08333333333333334\n",
      "The cos_similarity between doc0 and doc3 is 0.2182178902359924\n",
      "The cos_similarity between doc0 and doc4 is 0.13608276348795434\n",
      "The cos_similarity between doc0 and doc8 is 0.14433756729740646\n",
      "The cos_similarity between doc0 and doc9 is 0.12909944487358055\n",
      "The cos_similarity between doc1 and doc6 is 0.12909944487358055\n",
      "The cos_similarity between doc2 and doc3 is 0.545544725589981\n",
      "The cos_similarity between doc2 and doc8 is 0.14433756729740646\n",
      "The cos_similarity between doc2 and doc9 is 0.12909944487358055\n",
      "The cos_similarity between doc3 and doc8 is 0.1889822365046136\n",
      "The cos_similarity between doc3 and doc9 is 0.16903085094570328\n",
      "The cos_similarity between doc4 and doc5 is 0.5773502691896258\n",
      "The cos_similarity between doc4 and doc7 is 0.13608276348795434\n",
      "The cos_similarity between doc5 and doc7 is 0.23570226039551587\n",
      "The cos_similarity between doc8 and doc9 is 0.6708203932499369\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(i + 1, 10):\n",
    "        if cosine_similarity(BoW[i], BoW[j]) != 0:\n",
    "            print('The cos_similarity between doc{0} and doc{1} is {2}'.format(i, j, cosine_similarity(BoW[i], BoW[j])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity looks good; the similarity of two docs can be directly inferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare set-based and vector-based similarity calculation methods\n",
    "\n",
    "Set based similarity calculation is more efficient because the computation is done on the small piece of word sets.\n",
    "\n",
    "If the size of the document is large enough, vector based methods are more useful. However, the calculation amount will increase as the corpus increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
